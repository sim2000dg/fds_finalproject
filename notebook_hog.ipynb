{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.feature import hog\n",
    "import skimage.io as io\n",
    "from skimage.util import view_as_blocks\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "import pickle\n",
    "import numpy as np\n",
    "import warnings\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit encoders in order to map string labels to integers\n",
    "# The fact that we are storing the scikit objects makes the encoding reproducible and consistent\n",
    "# We do not need to fit each time, we store the fitted object and use its transform method when needed\n",
    "\n",
    "label_encoder_specific = LabelEncoder().fit(pd.read_csv('dataset_cards/cards.csv')['labels'])  # Specific\n",
    "label_encoder_category = LabelEncoder().fit(pd.read_csv('dataset_cards/cards.csv')['card type'])  # card type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage import color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class specifically aimed at digesting and retrieving the images of the cards, together with their\n",
    "    labels, which are also reliably encoded.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path: str, root_dir: str, transform: str,\n",
    "                 card_category: bool, label_encoder: LabelEncoder,\n",
    "                 subset: str = 'train') -> None:\n",
    "        \"\"\"\n",
    "        Initialization of the CardDataset class\n",
    "        :param csv_path: The path of the csv where to retrieve the necessary information about the images of the cards.\n",
    "        :param root_dir: The root directory for the dataset.\n",
    "        :param transform: Option between Histogram of Gradients ('hog') or Histogram of colors ('rgb_hist').\n",
    "        Pass None in order to avoid transformations (normalization at pixel level is the only operation performed).\n",
    "        :param card_category: Whether the label refers to the card category or to the specific card itself.\n",
    "        :param label_encoder: The scikit-learn LabelEncoder object necessary to map the string labels to integers.\n",
    "        :param subset: Choose between train ('train'), test ('test') and validation ('valid') dataset.\n",
    "        \"\"\"\n",
    "        total_cards = pd.read_csv(csv_path)\n",
    "        try:\n",
    "            self.cards_table = total_cards[total_cards['data set'] == subset]\n",
    "        except KeyError:\n",
    "            raise KeyError('There is no subset in the dataset with the queried label')\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            self.cards_table['labels' if not card_category else 'card type'] = \\\n",
    "                label_encoder.transform(self.cards_table['labels']) if not card_category else \\\n",
    "                label_encoder.transform(self.cards_table['card type'])\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.card_category = card_category\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cards_table)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        card_class = self.cards_table.iat[idx, 2] if not self.card_category else self.cards_table.iat[idx, 3]\n",
    "\n",
    "        if self.transform == 'hog':\n",
    "            representation = np.load(os.path.join(self.root_dir, 'hog',\n",
    "                                                  self.cards_table.iat[idx, 1].split('.')[0] + '.npy'))\n",
    "            representation = np.moveaxis(representation, 2, 0)\n",
    "            \n",
    "        elif self.transform == 'hog_2':\n",
    "            img_path = os.path.join(self.root_dir, self.cards_table.iat[idx, 1])\n",
    "            image = color.rgb2gray(io.imread(img_path))\n",
    "            fd, hog_image = hog(image, orientations=6, pixels_per_cell=(22, 22),\n",
    "                    cells_per_block=(1, 1), block_norm=\"L1\", visualize=True, feature_vector = True)\n",
    "            representation = fd\n",
    "            \n",
    "        elif self.transform is None or self.transform == 'rgb_hist':\n",
    "            img_path = os.path.join(self.root_dir, self.cards_table.iat[idx, 1])\n",
    "            image = io.imread(img_path)\n",
    "            if self.transform == 'rgb_hist':\n",
    "                representation, _ = np.histogramdd((np.reshape(image, (-1, 3))), bins=[np.linspace(0, 256, 6)] * 3)\n",
    "                representation = (representation / representation.sum()).flatten()\n",
    "            else:\n",
    "                representation = np.moveaxis(image/255.0, 2, 0)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f'There is no transformation function for \\'{self.transform}\\'')\n",
    "\n",
    "        return representation.astype(np.float32), card_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_hog = CardsDataset(csv_path=os.path.join('dataset_cards', 'cards.csv'), root_dir='dataset_cards',\n",
    "                            transform='hog_2', card_category=False,\n",
    "                            label_encoder=label_encoder_specific, subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array([i[0] for i in training_hog])\n",
    "# y = np.array([i[1] for i in training_hog])\n",
    "# np.save('hog.npy', X)\n",
    "# np.save('hogy.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('hog.npy')\n",
    "y = np.load('hogy.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7624, 600)\n",
      "(7624,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hog = CardsDataset(csv_path=os.path.join('dataset_cards', 'cards.csv'), root_dir='dataset_cards',\n",
    "                        transform='hog_2', card_category=False,\n",
    "                        label_encoder=label_encoder_specific, subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = np.array([i[0] for i in test_hog])\n",
    "y_t = np.array([i[1] for i in test_hog])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265, 600)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_t.shape)\n",
    "print(y_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 91 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iadig\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:  1.5min finished\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(multi_class=\"multinomial\", n_jobs=os.cpu_count()//2, solver=\"sag\", verbose=1).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.66%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', str(round(clf.score(X_t, y_t), 4)*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n",
      "[False False False  True False False  True  True  True  True  True  True\n",
      "  True  True  True False  True  True  True  True  True  True  True  True\n",
      "  True  True  True False  True False  True False  True  True  True  True\n",
      " False False False  True  True False False False False  True  True  True\n",
      "  True False  True False  True  True  True False False False False  True\n",
      " False False  True  True  True  True  True  True  True False  True  True\n",
      "  True  True  True  True  True False False  True  True False  True  True\n",
      "  True  True  True False  True False  True False  True  True  True  True\n",
      "  True False  True  True False False False False False  True  True  True\n",
      "  True False  True  True  True False  True False False  True  True  True\n",
      " False False  True  True False False  True  True False False  True  True\n",
      " False  True  True  True  True False False  True  True False  True  True\n",
      "  True False  True  True False  True  True  True False  True  True  True\n",
      "  True False False  True  True  True  True  True  True False  True False\n",
      "  True  True  True  True False False  True  True False  True  True  True\n",
      "  True  True  True  True  True False False  True  True False  True  True\n",
      " False  True  True False False  True  True  True  True  True  True  True\n",
      "  True  True  True  True False  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True False False False  True  True\n",
      " False False  True  True  True  True False  True  True False  True  True\n",
      " False False False  True False False False False False  True  True  True\n",
      "  True False  True False  True False False  True  True False  True False\n",
      " False]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_t)\n",
    "print((y_pred == y_t).sum())\n",
    "print(y_pred == y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "# from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = HistGradientBoostingClassifier(max_bins=224, max_iter=100).fit(X[0][0], y)\n",
    "# define the evaluation procedure\n",
    "# cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "# evaluate the model and collect the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_scores = cross_val_score(model, X_t, y_t, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "# print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
